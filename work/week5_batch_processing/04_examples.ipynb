{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "992209e4-fec9-4d8f-baa2-8db538bc26be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b791b02c-98f9-42db-a6cb-139c8116b680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f37b5c9-48e8-4872-82ef-21faec6de26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c356821a-0e87-4030-9794-c191575e9f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-02-26 22:00:30--  https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-01.csv\n",
      "Resolving nyc-tlc.s3.amazonaws.com (nyc-tlc.s3.amazonaws.com)... 52.217.42.52\n",
      "Connecting to nyc-tlc.s3.amazonaws.com (nyc-tlc.s3.amazonaws.com)|52.217.42.52|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 752335705 (717M) [text/csv]\n",
      "Saving to: ‘fhvhv_tripdata_2021-01.csv’\n",
      "\n",
      "fhvhv_tripdata_2021 100%[===================>] 717.48M  6.19MB/s    in 2m 8s   \n",
      "\n",
      "2022-02-26 22:02:39 (5.59 MB/s) - ‘fhvhv_tripdata_2021-01.csv’ saved [752335705/752335705]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-01.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed824211-018a-47d1-9516-ae285da3ae8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11908469 fhvhv_tripdata_2021-01.csv\n"
     ]
    }
   ],
   "source": [
    "!wc -l fhvhv_tripdata_2021-01.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba91c29e-8f99-417b-aa64-8eecca2fff3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hvfhs_license_num,dispatching_base_num,pickup_datetime,dropoff_datetime,PULocationID,DOLocationID,SR_Flag\n",
      "HV0003,B02682,2021-01-01 00:33:44,2021-01-01 00:49:07,230,166,\n",
      "HV0003,B02682,2021-01-01 00:55:19,2021-01-01 01:18:21,152,167,\n",
      "HV0003,B02764,2021-01-01 00:23:56,2021-01-01 00:38:05,233,142,\n",
      "HV0003,B02764,2021-01-01 00:42:51,2021-01-01 00:45:50,142,143,\n",
      "HV0003,B02764,2021-01-01 00:48:14,2021-01-01 01:08:42,143,78,\n",
      "HV0005,B02510,2021-01-01 00:06:59,2021-01-01 00:43:01,88,42,\n",
      "HV0005,B02510,2021-01-01 00:50:00,2021-01-01 01:04:57,42,151,\n",
      "HV0003,B02764,2021-01-01 00:14:30,2021-01-01 00:50:27,71,226,\n",
      "HV0003,B02875,2021-01-01 00:22:54,2021-01-01 00:30:20,112,255,\n"
     ]
    }
   ],
   "source": [
    "!head fhvhv_tripdata_2021-01.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40f047d5-8ec0-41d2-8cfe-01ceea67f3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/26 22:04:31 WARN Utils: Your hostname, HGDEWLFR001442 resolves to a loopback address: 127.0.1.1; using 172.17.181.93 instead (on interface eth0)\n",
      "22/02/26 22:04:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/konjufcad/spark/spark-3.0.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/02/26 22:04:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/26 22:04:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30b2f40a-7200-49cd-8bd8-37e58a2d6a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('fhvhv_tripdata_2021-01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c036e12-e516-4a8b-8eee-c5fe706e5be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(hvfhs_license_num,StringType,true),StructField(dispatching_base_num,StringType,true),StructField(pickup_datetime,StringType,true),StructField(dropoff_datetime,StringType,true),StructField(PULocationID,StringType,true),StructField(DOLocationID,StringType,true),StructField(SR_Flag,StringType,true)))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30aa11a5-3697-4758-ab47-8257d19a218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 101 fhvhv_tripdata_2021-01.csv > head.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b121d674-081a-4228-96a9-15f1c5eccead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 head.csv\n"
     ]
    }
   ],
   "source": [
    "!wc -l head.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3e4e021-901d-4e9a-ba90-2ebe8819f292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5197b92-9e9c-4673-98c1-e0cf928839a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = pd.read_csv('head.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b274570-9aca-4268-b74e-30b208c1f86f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hvfhs_license_num        object\n",
       "dispatching_base_num     object\n",
       "pickup_datetime          object\n",
       "dropoff_datetime         object\n",
       "PULocationID              int64\n",
       "DOLocationID              int64\n",
       "SR_Flag                 float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3b43113-a229-4cc4-9200-5d11cfed136d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(hvfhs_license_num,StringType,true),StructField(dispatching_base_num,StringType,true),StructField(pickup_datetime,StringType,true),StructField(dropoff_datetime,StringType,true),StructField(PULocationID,LongType,true),StructField(DOLocationID,LongType,true),StructField(SR_Flag,DoubleType,true)))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(df_pandas).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ea75100-f01b-4353-8a74-08c83032de18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57eccda4-2a97-4754-9d6c-88cd482b39e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('hvfhs_license_num',types.StringType(),True),\n",
    "    types.StructField('dispatching_base_num',types.StringType(),True),\n",
    "    types.StructField('pickup_datetime',types.TimestampType(),True),\n",
    "    types.StructField('dropoff_datetime',types.TimestampType(),True),\n",
    "    types.StructField('PULocationID',types.IntegerType(),True),\n",
    "    types.StructField('DOLocationID',types.IntegerType(),True),\n",
    "    types.StructField('SR_Flag',types.StringType(),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2718129-5478-46a2-98d6-dc8a57040060",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema2 = 'string,string,string,string,int,int,string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "181e657c-1d8a-4b58-bd1f-58a6ce0bf97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhvhv_tripdata_2021-01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "25c42bbd-70c0-4fba-a77f-662f54000183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f7e2503f-b31e-4dad-afb0-0e458075a672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 33, 44), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 49, 7), PULocationID=230, DOLocationID=166, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 55, 19), dropoff_datetime=datetime.datetime(2021, 1, 1, 1, 18, 21), PULocationID=152, DOLocationID=167, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 23, 56), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 38, 5), PULocationID=233, DOLocationID=142, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 42, 51), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 45, 50), PULocationID=142, DOLocationID=143, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 48, 14), dropoff_datetime=datetime.datetime(2021, 1, 1, 1, 8, 42), PULocationID=143, DOLocationID=78, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0005', dispatching_base_num='B02510', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 6, 59), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 43, 1), PULocationID=88, DOLocationID=42, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0005', dispatching_base_num='B02510', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 50), dropoff_datetime=datetime.datetime(2021, 1, 1, 1, 4, 57), PULocationID=42, DOLocationID=151, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 14, 30), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 50, 27), PULocationID=71, DOLocationID=226, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02875', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 22, 54), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 30, 20), PULocationID=112, DOLocationID=255, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02875', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 40, 12), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 53, 31), PULocationID=255, DOLocationID=232, SR_Flag=None)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6b531db6-9e22-4908-928b-8e372b618c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5d5271cf-8289-41d0-a5f7-892adaa6d6bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/home/konjufcad/code/de-course/work/week4_batch_processing/dhvhv/2021/01 already exists.;",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [47]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdhvhv/2021/01/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/readwriter.py:939\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    937\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m--> 939\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark/spark-3.0.3-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/utils.py:134\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    130\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: path file:/home/konjufcad/code/de-course/work/week4_batch_processing/dhvhv/2021/01 already exists.;"
     ]
    }
   ],
   "source": [
    "df.write.parquet('dhvhv/2021/01/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c9753141-da5d-4c2c-9be5-85ca0e653aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('dhvhv/2021/01/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "afd75066-79d1-487c-bbf8-7f0ad978f2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6356793d-fd1c-4786-b251-285f12dc2825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------+------------+\n",
      "|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "|2021-01-03 10:18:58|2021-01-03 10:44:37|         142|         138|\n",
      "|2021-01-01 17:07:47|2021-01-01 17:24:05|         213|         265|\n",
      "|2021-01-01 17:40:20|2021-01-01 17:54:23|         255|          97|\n",
      "|2021-01-01 16:05:08|2021-01-01 16:08:50|          62|          61|\n",
      "|2021-01-01 10:45:44|2021-01-01 10:53:22|          14|         228|\n",
      "|2021-01-01 10:32:57|2021-01-01 10:52:13|         181|         125|\n",
      "|2021-01-02 20:01:47|2021-01-02 20:31:26|          33|          35|\n",
      "|2021-01-03 01:53:03|2021-01-03 02:00:44|          60|         182|\n",
      "|2021-01-01 05:54:34|2021-01-01 06:18:28|          65|          39|\n",
      "|2021-01-01 17:35:19|2021-01-01 17:45:22|         141|         236|\n",
      "|2021-01-01 01:08:23|2021-01-01 01:25:36|          91|          76|\n",
      "|2021-01-01 16:52:13|2021-01-01 17:08:46|         134|         191|\n",
      "|2021-01-01 01:13:16|2021-01-01 01:26:21|         114|         246|\n",
      "|2021-01-03 13:12:13|2021-01-03 13:49:16|         228|         173|\n",
      "|2021-01-01 16:32:13|2021-01-01 16:38:04|         260|         260|\n",
      "|2021-01-01 03:55:12|2021-01-01 04:00:15|         157|          83|\n",
      "|2021-01-01 06:44:58|2021-01-01 06:53:52|          76|          77|\n",
      "|2021-01-01 00:39:49|2021-01-01 00:53:15|          76|          72|\n",
      "|2021-01-01 02:57:10|2021-01-01 03:15:52|          40|          17|\n",
      "|2021-01-02 12:03:34|2021-01-02 12:42:24|          63|          50|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('pickup_datetime','dropoff_datetime','PULocationID','DOLocationID') \\\n",
    ".filter(df.hvfhs_license_num == 'HV0003').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ecc9039b-19c0-4b2b-8dc4-593a08ac944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "47262e67-611a-41ef-9f37-909a00c40ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crazy_stuff(base_num):\n",
    "    num = int(base_num[1:])\n",
    "    if num % 7 == 0:\n",
    "        return f's/{num:03x}'\n",
    "    elif num % 3 == 0:\n",
    "        return f'a/{num:03x}'\n",
    "    else:\n",
    "        return f'e/{num:03x}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f377e209-5395-41a1-9314-a0f68ae3f14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s/b44'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crazy_stuff('B02884')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "169e5412-a0e4-4359-bbaf-1eb17a6efd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "crazy_stuff_udf = F.udf(crazy_stuff, returnType=types.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e693a5b6-a1a6-4f5c-a456-6999408aedc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f0cef3-7255-4d2a-beec-1397d938b5be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b6483b21-0bbb-492a-8146-94089dac3526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+------------+------------+\n",
      "|base_id|pickup_date|dropoff_date|PULocationID|DOLocationID|\n",
      "+-------+-----------+------------+------------+------------+\n",
      "|  s/b44| 2021-01-03|  2021-01-03|         142|         138|\n",
      "|  e/acc| 2021-01-01|  2021-01-01|         213|         265|\n",
      "|  e/9ce| 2021-01-01|  2021-01-01|         156|         147|\n",
      "|  e/acc| 2021-01-01|  2021-01-01|         255|          97|\n",
      "|  e/b38| 2021-01-01|  2021-01-01|          62|          61|\n",
      "|  a/b37| 2021-01-01|  2021-01-01|          14|         228|\n",
      "|  e/b3b| 2021-01-01|  2021-01-01|         181|         125|\n",
      "|  e/b48| 2021-01-02|  2021-01-02|          33|          35|\n",
      "|  e/b3f| 2021-01-03|  2021-01-03|          60|         182|\n",
      "|  e/acc| 2021-01-01|  2021-01-01|          65|          39|\n",
      "|  e/b3b| 2021-01-01|  2021-01-01|         141|         236|\n",
      "|  e/b38| 2021-01-01|  2021-01-01|          91|          76|\n",
      "|  a/b37| 2021-01-01|  2021-01-01|         134|         191|\n",
      "|  a/a7a| 2021-01-01|  2021-01-01|         114|         246|\n",
      "|  e/b3b| 2021-01-03|  2021-01-03|         228|         173|\n",
      "|  e/9ce| 2021-01-01|  2021-01-01|         254|          32|\n",
      "|  e/9ce| 2021-01-01|  2021-01-01|          63|          17|\n",
      "|  e/acc| 2021-01-01|  2021-01-01|         260|         260|\n",
      "|  e/9ce| 2021-01-02|  2021-01-02|          17|          61|\n",
      "|  s/b3d| 2021-01-01|  2021-01-01|         157|          83|\n",
      "+-------+-----------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/konjufcad/spark/spark-3.0.3-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/home/konjufcad/spark/spark-3.0.3-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/home/konjufcad/spark/spark-3.0.3-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 642, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/home/konjufcad/spark/spark-3.0.3-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    }
   ],
   "source": [
    "df\\\n",
    ".withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    ".withColumn('dropoff_date', F.to_date(df.dropoff_datetime)) \\\n",
    ".withColumn('base_id', crazy_stuff_udf(df.dispatching_base_num)) \\\n",
    ".select('base_id','pickup_date','dropoff_date','PULocationID','DOLocationID') \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412f906f-6213-44fd-8fa7-e082251201da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
